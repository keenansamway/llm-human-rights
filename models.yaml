

# Meta models
meta-llama/llama-4-maverick-17b-128e-instruct:
  save_id: llama-4-maverick-17b-128e-instruct
  model_name: Llama 4 Maverick
  provider: ["lambda/fp8"] # lambda/fp8, klusterai/fp8
  temperature: 0.6
  logprobs: true

meta-llama/llama-4-scout:
  save_id: llama-4-scout
  model_name: Llama 4 Scout
  provider: ["centml/bf16"]
  temperature: 0.6

meta-llama/llama-3.3-70b-instruct:
  save_id: llama-3.3-70b-instruct
  model_name: Llama 3.3 70B Instruct
  provider: ["nebius/fp8"]
  temperature: 0.6
  logprobs: true

meta-llama/llama-3.1-8b-instruct:
  save_id: llama-3.1-8b-instruct
  model_name: Llama 3.1 8B Instruct
  provider: ["cerebras/fp16"]
  temperature: 0.6


# Qwen models
qwen/qwen3-235b-a22b:
  save_id: qwen3-235b-a22b
  model_name: Qwen 3 235B A22B
  provider: ["nebius/fp8"]
  temperature: 0.6

qwen/qwen3-32b:
  save_id: qwen3-32b
  model_name: Qwen 3 32B
  provider: ["nebius/base"]
  temperature: 0.6
  # logprobs: true

qwen/qwen3-14b:
  save_id: qwen3-14b
  model_name: Qwen 3 14B
  provider: ["nebius/fp8"]
  temperature: 0.6

qwen/qwen3-8b:
  save_id: qwen3-8b
  model_name: Qwen 3 8B
  provider: ["novita/fp8"]
  temperature: 0.6

qwen/qwen-2.5-72b-instruct:
  save_id: qwen-2.5-72b-instruct
  model_name: Qwen 2.5 72B
  provider: ["nebius/fp8"]
  temperature: 0.6
  logprobs: true


# DeepSeek models
deepseek/deepseek-chat-v3-0324:
  save_id: deepseek-chat-v3-0324
  model_name: DeepSeek Chat V3 0324
  provider: ["nebius/fp8"]
  temperature: 0.6
  logprobs: true

deepseek/deepseek-r1-0528:
  save_id: deepseek-r1-0528
  model_name: DeepSeek R1 0528
  provider: ["nebius/fp8"]
  temperature: 0.6
  logprobs: true

deepseek/deepseek-r1-distill-llama-8b:
  save_id: deepseek-r1-distill-llama-8b
  model_name: R1 Llama 8B
  provider: ["novita"]
  temperature: 0.6

# xAI models
x-ai/grok-4:
  save_id: grok-4
  model_name: Grok 4
  provider: ["xai"]
  temperature: 0.6

x-ai/grok-3:
  save_id: grok-3
  model_name: Grok 3
  provider: ["xai"]
  temperature: 0.6

x-ai/grok-3-mini:
  save_id: grok-3-mini
  model_name: Grok 3 Mini
  provider: ["xai"]
  temperature: 0.6


# Anthropic models
anthropic/claude-sonnet-4:
  save_id: claude-sonnet-4
  model_name: Claude Sonnet 4
  provider: ["google-vertex"]
  temperature: 0.6

anthropic/claude-3.5-sonnet:
  save_id: claude-3.5-sonnet
  model_name: Claude 3.5 Sonnet v2
  provider: ["google-vertex"]
  temperature: 0.6


# Mistral models
mistralai/mistral-medium-3:
  save_id: mistral-medium-3
  model_name: Mistral Medium 3
  provider: ["mistral"]
  temperature: 0.6

mistralai/mixtral-8x22b-instruct:
  save_id: mixtral-8x22b-instruct
  model_name: Mixtral 8x22B Instruct
  provider: ["fireworks"]
  temperature: 0.6
  logprobs: true

mistralai/mistral-7b-instruct-v0.1:
  save_id: mistral-7b-instruct-v0.1
  model_name: Mistral 7B Instruct v0.1
  provider: ["Together"]
  temperature: 0.6


# Google models
google/gemini-2.5-pro:
  save_id: gemini-2.5-pro
  model_name: Gemini 2.5 Pro
  provider: ["google-ai-studio"]
  temperature: 0.6

google/gemini-2.5-flash-preview-05-20:
  save_id: gemini-2.5-flash-preview-05-20
  model_name: Gemini 2.5 Flash 05-20
  provider: ["google-ai-studio"]
  temperature: 0.6

google/gemma-3-27b-it:
  save_id: gemma-3-27b-it
  model_name: Gemma 3 27B
  provider: ["deepinfra/bf16"]
  temperature: 0.6

google/gemma-3-12b-it:
  save_id: gemma-3-12b-it
  model_name: Gemma 3 12B
  provider: ["deepinfra/bf16"]
  temperature: 0.6

google/gemma-3-4b-it:
  save_id: gemma-3-4b-it
  model_name: Gemma 3 4B
  provider: ["deepinfra/bf16"]
  temperature: 0.6


# OpenAI models
openai/o3:
  save_id: o3
  model_name: o3
  provider: ["openai"]
  temperature: 1.0

openai/gpt-4.1-2025-04-14:
  save_id: gpt-4.1-2025-04-14
  model_name: GPT-4.1
  provider: ["openai"]
  temperature: 0.6
  logprobs: true

openai/gpt-4o-2024-11-20:
  save_id: gpt-4o 1120
  model_name: GPT-4o 1120
  provider: ["openai"]
  temperature: 0.6
  logprobs: true

openai/gpt-3.5-turbo-0125:
  save_id: gpt-3.5-turbo-0125
  model_name: GPT-3.5 Turbo 0125
  provider: ["openai"]
  temperature: 0.6
  logprobs: true